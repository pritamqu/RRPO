<!DOCTYPE html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
    google.load("jquery", "1.3.2");
</script>

<!-- 
    quick html converter
    https://htmled.it/
 -->
<html>

<head>
    <title>RRPO</title>
    <meta property="og:title"
        content="Self-Alignment of Large Video Language Models with Refined Regularized Preference Optimization" />
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>

<body>
    <br>
    <center>
        <span style="font-size:34px">Self-Alignment of Large Video Language Models with Refined Regularized Preference Optimization</span>
        <br><br>
        <span style="font-size:20px">Preprint. Under review.</span>

        <br><br>
        <table align=center>
            <tr>
                <span style="font-size:20px"><a href="https://www.pritamsarkar.com">Pritam Sarkar</a></span> 
                &nbsp; <span style="font-size:20px"><a href="https://www.aiimlab.com/ali-etemad">Ali Etemad</a></span>
            </tr>
        </table>
        

        <table align="center" style="margin: 20px auto; border-spacing: 40px 10px;">
            <tr>
              <td align="center">
                <div style="font-size: 24px; color: #444;"><i class="fas fa-file-alt"></i></div>
                <a href="https://arxiv.org/abs/2504.12083" style="font-size: 18px; color: #007acc; text-decoration: none;">Paper</a>
              </td>
              <td align="center">
                <div style="font-size: 24px; color: #444;"><i class="fas fa-globe"></i></div>
                <a href="https://pritamqu.github.io/RRPO/" style="font-size: 18px; color: #007acc; text-decoration: none;">Website</a>
              </td>
              <td align="center">
                <div style="font-size: 24px; color: #444;"><i class="fab fa-github"></i></div>
                <a href="https://github.com/pritamqu/RRPO" style="font-size: 18px; color: #007acc; text-decoration: none;">Code</a>
              </td>
              <td align="center">
                <div style="height: 24px;"><img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="height: 24px;"></div>
                <a href="https://huggingface.co/collections/pritamqu/rrpo-67fbc8c048b298a5fdfb167b" style="font-size: 18px; color: #007acc; text-decoration: none;">Models</a>
              </td>
              <td align="center">
                <div style="font-size: 24px; color: #444;"><i class="fas fa-database"></i></div>
                <a href="https://huggingface.co/datasets/pritamqu/self-alignment" style="font-size: 18px; color: #007acc; text-decoration: none;">Data</a>
              </td>
            </tr>
          </table>
    	  

    <!--------------------- Abstract --------------------->
	<!-- <table align=center>
        <center>
            <h1>
                Abstract
            </h1>
        </center>
        <tr>
            <p style="text-align: justify;">
            Despite recent advances in Large Video Language Models (LVLMs), they still struggle with fine-grained temporal understanding, hallucinate, and often make simple mistakes on even simple video question-answering tasks, all of which pose significant challenges to their safe and reliable deployment in real-world applications. To address these limitations, we propose a self-alignment framework that enables LVLMs to learn from their own errors. Our proposed framework first obtains a training set of preferred and non-preferred response pairs, where non-preferred responses are generated by incorporating common error patterns that often occur due to inadequate spatio-temporal understanding, spurious correlations between co-occurring concepts, and over-reliance on linguistic cues while neglecting the vision modality, among others. To facilitate self-alignment of LVLMs with the constructed preferred and non-preferred response pairs, we introduce Refined Regularized Preference Optimization (RRPO), a novel preference optimization method that utilizes sub-sequence-level refined rewards and token-wise KL regularization to address the limitations of Direct Preference Optimization (DPO). We demonstrate that RRPO achieves more precise alignment and more stable training compared to DPO. Our experiments and analysis validate the effectiveness of our approach across diverse video tasks, including video hallucination, short- and long-video understanding, and fine-grained temporal reasoning. 

            </p>
        </tr>
    </table>
    <br> -->

    <!-- <br> -->
    <table border="0" style="width: 100%; border-collapse: collapse; background-color: #CCCCFF; border-radius: 10px;">
      <tbody>
        <tr>
          <td style="text-align: center; padding: 10px;"><strong> Our contributions </strong>
            <ul style="text-align: left;">
            <!-- <li> -->
            &#128073;
                We design a <strong>self-alignment</strong> framework to facilitate self-improvement of LVLMs based on their own errors.
                We introduce <strong>RRPO</strong>, a preference optimization method that addresses the limitations of DPO by utilizing sub-sequence-level refined rewards and token-wise strong KL regularizer, resulting in more precise alignment and stable training.
            <!-- </li> -->
            <!-- <li> -->
            <br><br>
            &#128073;
            Our rigorous evaluation demonstrates the effectiveness of our proposed method across diverse video tasks, including video hallucination, short and long video understanding, and fine-grained temporal reasoning, among others. Moreover, our experimental and theoretical analysis highlight the superiority of RRPO over DPO in aligning LVLMs. 
            <!-- </li> -->
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <br><br>
    
    <!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->


    <!-- <table border="0" style="width: 100%; height: 40px; border-collapse: collapse; background-color: #FAFAD2; border-radius: 10px; padding: 30px; text-align: center;">
      <tbody>
        <tr>
          <td style="font-size: 20px; font-family: Arial, sans-serif;">
            An overview of our self-alignment framework.          </td>
        </tr>
      </tbody>
    </table>
    <br> -->


    <table border="0" style="border-collapse: collapse; width: 100%;">
    <tbody>
    <tr>
    <td style="text-align: center;"><img src="./assets/self-alignment.png" width="100%" height="auto" /></td>
    </tr>
    <!-- <tr> -->
    <td style="width: 33.33%; height: 5px; vertical-align:top; text-align: center">An overview of our self-alignment framework.</td>
    <!-- </tr> -->
    </tbody>
    </table>
    <br>

    <table border="0" style="border-collapse: collapse; width: 100%;">
      <tbody>
      <tr>
      <td style="width: 33.33%;"><img src="./assets/perturb.png" width="100%" height="100%" /></td>
      <td style="width: 33.33%;"><img src="./assets/data.png" width="100%" height="100%" /></td>
      </tr>
      <tr>
      <td style="width: 33.33%; height: 5px; vertical-align:top; text-align: center">An example of perturbed video.</td>
      <td style="width: 33.33%; height: 5px; vertical-align:top; text-align: center">A few training samples.</td>
      </tr>
      </tbody>
      </table>
      <br>

    <br>
    <table border="0" style="width: 100%; border-collapse: collapse; background-color: #7DF9FF; border-radius: 10px;">
      <tbody>
        <tr>
          <td style="text-align: center; padding: 10px;"><strong> An Overview of Refined Regularized Preference Optimization </strong>
            <ul style="text-align: left;">
              <!-- <li> -->
                <p>
                  Given an input \(x\) with a pair of responses \(\{y^+, y^-\}\), where \(y^+ \succ y^- | x\), 
                  we align \(\pi_\theta\) to favor \(y^+\) over \(y^-\). 
                  RRPO training objective is defined as:
                </p>
                <p>
                  \[
                  \mathcal{L}_\text{RRPO}(\pi_{\theta};\pi_\text{ref}) =
                  -\mathbb{E}_{(x,y^+,y^-) \sim \mathcal{D}} 
                  \left[ 
                  \log \sigma (u) 
                  + \alpha \cdot \mathbb{D}_{\text{TKL}} \big(x,y^+\big)
                  \right]
                  \]
                </p>
                <p>
                  the total reward margin \(u\) is defined as:
                  \[
                  u = \sum\limits_{i=1}^{N} u_i 
                  = \sum\limits_{i=1}^{N} \bigl( r_\theta(x, y^+_i) - r_\theta(x, y^-_i) \bigr)
                  \]
                </p>
                <p>
                  the reward for \(i^{th}\) phrase \(r_\theta(x, y_i)\) is defined as:
                  \[
                  r_\theta(x, y_i) = 
                  \beta \log \left(
                    \frac{
                      \prod\limits_{j=s_i}^{e_i} \pi_\theta(t_j \mid x, t_{\lt j})
                    }
                    {
                      \prod\limits_{j=s_i}^{e_i} \pi_{\text{ref}}(t_j \mid x, t_{\lt j})
                    }
                  \right)
                  \]
                  where \({s_i}\) and \({e_i}\) are the start and end token indices of \(i^{th}\) phrase
                </p>
                <p>
                  the token-wise KL regularizer \(\mathbb{D}_{\text{TKL}}\) is defined as:
                  \[
                  \mathbb{D}_{\text{TKL}} \big(x,y^+;\pi_{\text{ref}} \,\|\, \pi_{\theta} \big) =
                  \sum\limits_{t=1}^{|y^+|} 
                  \mathbb{D}_{\text{KL}} \left( 
                    \pi_{\text{ref}} (\cdot \mid [x, y^+_{\lt t}]) \,\|\, 
                    \pi_{\theta} (\cdot \mid [x, y^+_{\lt t}]) 
                  \right)
                  \]
                </p>
                
              <!-- </li> -->
            </ul>
          </td>
        </tr>
      </tbody>
    </table>

    <!-- <table border="0" style="width: 100%; height: 100px; border-collapse: collapse; background-color: #FAFAD2; border-radius: 10px; padding: 30px; text-align: center;">
      <tbody>
        <tr>
          
        </tr>
      </tbody>
    </table>
    <br> -->

    <table border="1" style="border-collapse: collapse; width: 100%; font-size: 14px;">
      <caption style="caption-side: top; font-weight: bold; padding: 8px;">
        Comparison with existing preference optimization methods.
        <!-- <br>

        Δ = (1/N) ∑(acc<sub>aligned</sub> − acc<sub>base</sub>),
        %Δ = (100/N) ∑(acc<sub>aligned</sub> − acc<sub>base</sub>) / acc<sub>base</sub>
        <br>
        where N is the number of evaluation benchmarks used for each ablation.
        <br> -->
        RRPO consistently outperforms existing alignment methods.
      </caption>
      <thead>
        <tr>
          <th></th>
          <th><b>TVBench</b></th>
          <th><b>VideoHallucer</b></th>
          <th><b>VideoMME</b></th>
          <th><b>MLVU</b></th>
          <th><b>Δ / %Δ</b></th>
        </tr>
      </thead>
      <tbody>
        <tr style="background-color: #f0fdf4;">
          <td><b>LongVU<sub>7B</sub> (base)</b></td>
          <td>53.7</td>
          <td>39.2</td>
          <td>56.2</td>
          <td>63.6</td>
          <td>–</td>
        </tr>
        <tr>
          <td>+ DPO</td>
          <td>54.3</td>
          <td>40.9</td>
          <td>56.6</td>
          <td>63.6</td>
          <td>0.7 / 1.5</td>
        </tr>
        <tr>
          <td>+ DPA</td>
          <td>54.6</td>
          <td>40.3</td>
          <td>56.9</td>
          <td>63.9</td>
          <td>0.7 / 1.5</td>
        </tr>
        <tr>
          <td>+ TDPO</td>
          <td>53.9</td>
          <td>41.4</td>
          <td>57.0</td>
          <td>63.8</td>
          <td>0.8 / 1.9</td>
        </tr>
        <tr>
          <td>+ DDPO</td>
          <td>54.2</td>
          <td>41.7</td>
          <td>56.7</td>
          <td>63.6</td>
          <td>0.9 / 2.0</td>
        </tr>
        <!-- <tr>
          <td>+ RRPO w/o R*</td>
          <td>54.3</td>
          <td>43.0</td>
          <td><b>57.8</b></td>
          <td><b>64.5</b></td>
          <td>1.7 / 3.8</td>
        </tr>
        <tr>
          <td>+ RRPO w/o D<sub>TKL</sub></td>
          <td>54.9</td>
          <td>39.1</td>
          <td>57.4</td>
          <td>63.9</td>
          <td>0.6 / 1.1</td>
        </tr> -->
        <tr style="background-color: #f0fdf4;">
          <td><b>+ RRPO (ours)</b></td>
          <td><b>56.5</b></td>
          <td><b>44.0</b></td>
          <td><b>57.7</b></td>
          <td><b>64.5</b></td>
          <td><b>2.5 / 5.4</b></td>
        </tr>
      </tbody>
    </table>
    <br> <br>
    <table border="0" style="border-collapse: collapse; width: 100%;">
      <tbody>
      <tr>
      <td style="text-align: center;"><img src="./assets/postalignment_divergence.png" width="40%" height="auto" /></td>
      </tr>
      <!-- <tr> -->
      <td style="width: 33.33%; height: 5px; vertical-align:top; text-align: center">Performance relative to model divergence. RRPO exhibits the best performance-
        divergence trade-off.</td>
      <!-- </tr> -->
      </tbody>
      </table>

      <br> <br>
      <table border="1" style="border-collapse: collapse; width: 100%; font-size: 14px; text-align: center;">
      
        <caption style="caption-side: top; font-weight: bold; padding: 8px;">
          RRPO shows consistent improvements over the base model and outperforms DPO across all benchmarks on diverse video tasks.
        </caption>
      
        <thead>
          <tr>
            <th><strong>Models</strong></th>
            <th><strong>#F</strong></th>
            <th><strong>TV Bench</strong></th>
            <th><strong>Temp Compass</strong></th>
            <th><strong>Video Hallucer</strong></th>
            <th><strong>Vid Halluc</strong></th>
            <th><strong>MV Bench</strong></th>
            <th><strong>Video MME</strong></th>
            <th><strong>MLVU</strong></th>
            <th><strong>LongVideo Bench</strong></th>
          </tr>
        </thead>
        <tbody>
          <tr style="background-color: #cce5ff;">
            <td>VideoChat2</td>
            <td>16</td>
            <td>44.0</td>
            <td>59.3</td>
            <td>23.1</td>
            <td>73.3</td>
            <td><strong>60.2</strong></td>
            <td>41.0</td>
            <td>46.4</td>
            <td>40.4</td>
          </tr>
          <tr>
            <td>VideoChat2 + DPO</td>
            <td>16</td>
            <td>45.7</td>
            <td>60.0</td>
            <td>22.1</td>
            <td>72.4</td>
            <td>59.6</td>
            <td>43.0</td>
            <td>47.4</td>
            <td>41.0</td>
          </tr>
          <tr style="background-color: #cce5ff;">
            <td>VideoChat2 + <strong>RRPO</strong></td>
            <td>16</td>
            <td><strong>45.8</strong></td>
            <td><strong>60.2</strong></td>
            <td><strong>32.9</strong></td>
            <td><strong>76.4</strong></td>
            <td>59.0</td>
            <td><strong>44.3</strong></td>
            <td><strong>47.9</strong></td>
            <td><strong>42.8</strong></td>
          </tr>
          <tr><td colspan="10">&nbsp;</td></tr>
          <tr style="background-color: #e6e6e6;">
            <td>LLaVA-Video</td>
            <td>64</td>
            <td>51.0</td>
            <td>66.0</td>
            <td>50.0</td>
            <td>76.6</td>
            <td>61.1</td>
            <td>64.0</td>
            <td>68.6</td>
            <td>60.1</td>
          </tr>
          <tr>
            <td>LLaVA-Video + DPO</td>
            <td>64</td>
            <td>51.9</td>
            <td>66.4</td>
            <td>53.3</td>
            <td>76.5</td>
            <td>60.6</td>
            <td>63.1</td>
            <td>67.4</td>
            <td>59.4</td>
          </tr>
          <tr style="background-color: #e6e6e6;">
            <td>LLaVA-Video + <strong>RRPO</strong></td>
            <td>64</td>
            <td>51.9</td>
            <td>66.8</td>
            <td><strong>55.7</strong></td>
            <td>76.5</td>
            <td><strong>62.2</strong></td>
            <td><strong>64.5</strong></td>
            <td>69.1</td>
            <td><strong>60.4</strong></td>
          </tr>
          <tr style="background-color: #e6e6e6;">
            <td>LLaVA-Video + <strong>RRPO</strong> (32f)</td>
            <td>64</td>
            <td><strong>52.2</strong></td>
            <td><strong>67.4</strong></td>
            <td><strong>55.8</strong></td>
            <td><strong>76.6</strong></td>
            <td>62.1</td>
            <td><strong>64.5</strong></td>
            <td><strong>69.4</strong></td>
            <td>60.1</td>
          </tr>
          <tr><td colspan="10">&nbsp;</td></tr>
          <tr style="background-color: #dff0d8;">
            <td>LongVU</td>
            <td>1fps</td>
            <td>53.7</td>
            <td>63.9</td>
            <td>39.2</td>
            <td>67.3</td>
            <td>65.5</td>
            <td>56.2</td>
            <td>63.6</td>
            <td>48.6</td>
          </tr>
          <tr>
            <td>LongVU + DPO</td>
            <td>1fps</td>
            <td>54.3</td>
            <td>64.3</td>
            <td>40.9</td>
            <td>68.5</td>
            <td>65.9</td>
            <td>56.6</td>
            <td>63.6</td>
            <td>49.4</td>
          </tr>
          <tr style="background-color: #dff0d8;">
            <td>LongVU + <strong>RRPO</strong></td>
            <td>1fps</td>
            <td><strong>56.5</strong></td>
            <td><strong>64.5</strong></td>
            <td><strong>44.0</strong></td>
            <td><strong>71.7</strong></td>
            <td><strong>66.8</strong></td>
            <td><strong>57.7</strong></td>
            <td><strong>64.5</strong></td>
            <td><strong>49.7</strong></td>
          </tr>
        </tbody>
      </table>
      

    <br> <br>

    <table border="0" style="width: 100%; height: 100px; border-collapse: collapse; background-color: #FAFAD2; border-radius: 10px; padding: 30px; text-align: center;">
      <tbody>
        <!-- <tr>
          
        </tr> -->

        <td style="text-align: center; padding: 10px;"><strong> Highlights </strong>
          <ul style="text-align: left;">
          <!-- <li> -->
          &#128073;
          RRPO is more stable and highly effective compared to prior and concurrent preference optimization methods.
          <br>
          &#128073;
          The fine-grained reward modeling in RRPO improves capabilities without diverging away from its initial state, thus preserve the valuable prior knowledge.
          <!-- </li> -->
          <br>
          &#128073;
          RRPO scales effectively with more data and high-resolution inputs, and generalizes well across diverse LVLMs.
          <br>
          &#128073;
          RRPO exhibits consistent improvements across all setups over the base models in diverse video tasks.
          </ul>
        </td>
      </tbody>
    </table>
    <br>

    <!--------------------- citation --------------------->
    <h1>
                Read our paper for more insights!
    </h1>

    <embed
    src="https://arxiv.org/pdf/2504.12083.pdf"
    width=100% height="600px" />

    <!--------------------- citation --------------------->
    <table align=center>
        <center>
            <h1>
                Citation
            </h1>
        </center>
        <tr>
            <p style="text-align: center;">
            Please cite our paper using the given BibTeX entry.
            </p>
        </tr>
    </table>

  <textarea id="bibtexEntry" readonly style="width: 100%; height: 134px; background-color: #faf9f6;">
    @misc{sarkar2025rrpo,
      title={Self-Alignment of Large Video Language Models with Refined Regularized Preference Optimization}, 
      author={Pritam Sarkar and Ali Etemad},
      year={2025},
      eprint={2504.12083},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
    }
  </textarea>
  <button onclick="copyToClipboard()">Copy BibTeX</button>

  <script>
    function copyToClipboard() {
      const textarea = document.getElementById("bibtexEntry");
      textarea.select();
      textarea.setSelectionRange(0, 99999);
      document.execCommand("copy");
      alert("BibTeX entry copied!");
    }
  </script>



	<br> <br> <hr>

    <!--------------------- Question --------------------->    

    <table align=center width=950px>
        <center>
            <h1>
                Contact me:
            </h1>
        </center>
        <tr>
            <p>
                You may directly contact me at <a href="mailto:pritam.sarkar@queensu.ca">pritam.sarkar@queensu.ca</a> or connect with me on <a href="https://www.linkedin.com/in/sarkarpritam/">LinkedIn</a>. <br>
                &#11088 <strong>I am on the job market for a full-time role as a researcher. If you find my experience a good fit, please reach out.</strong> &#11088
            </p>
        </tr>
    </table>

</html>
